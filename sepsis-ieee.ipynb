{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"path = '../input/sepsisdata/training'\npath_a = '../input/sepsisdata/training/training_setA'\npath_b = '../input/sepsisdata/training/training_setB'\n","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = []\nX_train = []\n\nfor i in os.listdir(path_a):\n    data = pd.read_csv(path_a+'/'+i,sep = '|')\n    data.drop(['EtCO2','Fibrinogen', 'Unit1', 'Unit2', 'BaseExcess', 'DBP', 'Hct', 'Hgb', 'PTT', 'WBC', 'pH','HCO3','FiO2', 'PaCO2', 'Platelets', 'Magnesium',  'Phosphate',  'Potassium', 'Bilirubin_total',  'TroponinI','SaO2', 'AST','BUN', 'Alkalinephos', 'Bilirubin_direct','Glucose','Lactate', 'Calcium',  'Chloride', 'Creatinine' ],axis = 1,inplace = True)\n\n    data.dropna(thresh=data.shape[1]*0.40,how='all',inplace = True)\n    La_1 = data['SepsisLabel'].sum()\n    if La_1:\n        y_train.append(1)\n    else:\n        y_train.append(0)\n    data.drop(['SepsisLabel'],axis = 1,inplace = True)\n    data = data.apply(lambda x: x.fillna(x.median()),axis=0)\n    data = data.fillna(0)\n    if len(data) < 40:\n        Pad = pd.DataFrame({'HR':0.0 ,'O2Sat':0.0, 'Temp':0.0 , 'SBP':0.0, 'MAP':0.0, 'Resp':0.0, 'Age':0.0, 'Gender': 0 ,'HospAdmTime':0.0, 'ICULOS':0}, index =[item for item in range(0,40-len(data))])\n        data = pd.concat([Pad, data]).reset_index(drop = True)\n    elif len(data) >40:\n        data = data[len(data)-40::1]\n    data = data.values\n    X_train.append(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in os.listdir(path_b):\n    data = pd.read_csv(path_b+'/'+i,sep = '|')\n    data.drop(['EtCO2','Fibrinogen', 'Unit1', 'Unit2', 'BaseExcess', 'DBP', 'Hct', 'Hgb', 'PTT', 'WBC', 'pH','HCO3','FiO2', 'PaCO2', 'Platelets', 'Magnesium',  'Phosphate',  'Potassium', 'Bilirubin_total',  'TroponinI','SaO2', 'AST','BUN', 'Alkalinephos', 'Bilirubin_direct','Glucose','Lactate', 'Calcium',  'Chloride', 'Creatinine' ],axis = 1,inplace = True)\n\n    data.dropna(thresh=data.shape[1]*0.40,how='all',inplace = True)\n    La_1 = data['SepsisLabel'].sum()\n    if La_1:\n        y_train.append(1)\n    else:\n        y_train.append(0)\n    data.drop(['SepsisLabel'],axis = 1,inplace = True)\n    data = data.apply(lambda x: x.fillna(x.median()),axis=0)\n    data = data.fillna(0)\n    if len(data) < 40:\n        Pad = pd.DataFrame({'HR':0.0 ,'O2Sat':0.0, 'Temp':0.0 , 'SBP':0.0, 'MAP':0.0, 'Resp':0.0, 'Age':0.0, 'Gender': 0 ,'HospAdmTime':0.0, 'ICULOS':0}, index =[item for item in range(0,40-len(data))])\n        data = pd.concat([Pad, data]).reset_index(drop = True)\n    elif len(data) >40:\n        data = data[len(data)-40::1]\n    data = data.values\n    X_train.append(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = np.array(X_train) , np.array(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.25, random_state=45)\nprint(len(X_train_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_train_))\nprint(len(X_test_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(40,10), return_sequences = True))\nmodel.add(LSTM(256))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(X_train_, y_train_, epochs=10, batch_size=32, verbose=1, validation_split=0.2, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\n#model.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfrom keras.models import model_from_json\n# load json and create model\njson_file = open('/kaggle/working/model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"/kaggle/working/model.h5\")\nprint(\"Loaded model from disk\")'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = loaded_model.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom scipy import stats\n\ndist = Counter(y)\nfor k in dist:\n    dist[k] /= len(X)\n\nacum = 0\nbound = {}\nfor i in range(1):\n    acum += dist[i]\n    bound[i] = np.percentile(y_pred, acum * 100)\nprint(bound)\n\ndef classify(x):\n    if x <= bound[0]:\n        return 0\n    else:\n        return 1\n    \nfinal_pred = np.array(list(map(classify, y_pred)))\nprint(final_pred)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test_, final_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueValues, occurCount = np.unique(y_pred, return_counts=True)\noccurCount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uniqueValues, occurCount = np.unique(y_test_, return_counts=True)\noccurCount","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}